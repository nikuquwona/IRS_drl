from matplotlib import pyplot as plt
import torch
import numpy as np
from torch.utils.tensorboard import SummaryWriter
import gym
import argparse
from normalization import Normalization, RewardScaling
from replaybuffer import ReplayBuffer
from ppo_continuous import PPO_continuous ,Actor_Gaussian,Critic
from env import env as ENV

def evaluate_policy(args, env, agent, state_norm):
    times = 3
    evaluate_reward = 0
    for _ in range(times):
        s = env.reset()
        if args.use_state_norm:
            s = state_norm(s, update=False)  # During the evaluating,update=False
        done = False
        episode_reward = 0
        step=0
        while not done:
            a = agent.evaluate(s)  # We use the deterministic policy during the evaluating
            if args.policy_dist == "Beta":
                action = 2 * (a - 0.5) * args.max_action  # [0,1]->[-max,max]
            else:
                action = a
            s_, r, done, _ = env.step(action)
            step+=1
            if step==args.max_episode_steps:
                done=True
            if args.use_state_norm:
                s_ = state_norm(s_, update=False)
            episode_reward += r
            s = s_
        evaluate_reward += episode_reward

    return evaluate_reward / times


def main(args, env_name, number, seed):
    env = ENV()#gym.make(env_name)
    env_evaluate = ENV()#gym.make(env_name)  # When evaluating the policy, we need to rebuild an environment
    # Set random seed
    # env.seed(seed)
    # env.action_space.seed(seed)
    # env_evaluate.seed(seed)
    # env_evaluate.action_space.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)

    args.state_dim = 13#8#env.observation_space.shape[0]
    args.action_dim = 13#8#env.action_space.shape[0]
    args.max_action = 1#float(env.action_space.high[0]) # -1,1
    args.max_episode_steps = 10000#env._max_episode_steps  # Maximum number of steps per episode
    print("env={}".format(env_name))
    print("state_dim={}".format(args.state_dim))
    print("action_dim={}".format(args.action_dim))
    print("max_action={}".format(args.max_action))
    print("max_episode_steps={}".format(args.max_episode_steps))

    evaluate_num = 0  # Record the number of evaluations
    evaluate_rewards = []  # Record the rewards during the evaluating
    total_steps = 0  # Record the total steps during the training

    replay_buffer = ReplayBuffer(args)
    agent = PPO_continuous(args)
    ## load
        # ä¿å­˜å’ŒåŠ è½½æ¨¡å‹è¿˜æ²¡æœ‰

    ##
    # Build a tensorboard
    writer = SummaryWriter(log_dir='runs/PPO_continuous/env_{}_{}_number_{}_seed_{}'.format(env_name, args.policy_dist, number, seed))

    state_norm = Normalization(shape=args.state_dim)  # Trick 2:state normalization
    if args.use_reward_norm:  # Trick 3:reward normalization
        reward_norm = Normalization(shape=1)
    elif args.use_reward_scaling:  # Trick 4:reward scaling
        reward_scaling = RewardScaling(shape=1, gamma=args.gamma)

    ans_max=-999
    ans_state=[]
    ###
    reward_record=[]
    path=[]
    ###
    while total_steps < args.max_train_steps:
        s = env.reset()
        path=[]
        path.append(s.tolist())

        if args.use_state_norm:
            s = state_norm(s)
        if args.use_reward_scaling:
            reward_scaling.reset()
        episode_steps = 0
        done = False
        while not done:
            episode_steps += 1
            a, a_logprob = agent.choose_action(s)  # Action and the corresponding log probability
            print("a",a,"a_logprob",a_logprob)
            if args.policy_dist == "Beta":
                action = 2 * (a - 0.5) * args.max_action  # [0,1]->[-max,max]
            else:
                action = a
            s_, r, done, _ = env.step(action)

            ###
            if total_steps>=args.max_train_steps-args.max_episode_steps:
                reward_record.append(r)
            path.append(s_.tolist())
            # print('path',path)
            state_to_print=s_
            if episode_steps==args.max_episode_steps:
                done =True
                if r>ans_max:
                    ans_max=r
                    ans_state=state_to_print
            ###

            if args.use_state_norm:
                s_ = state_norm(s_)
            if args.use_reward_norm:
                r = reward_norm(r)
            elif args.use_reward_scaling:
                r = reward_scaling(r)

            
            # When dead or win or reaching the max_episode_steps, done will be Ture, we need to distinguish them;
            # dw means dead or win,there is no next state s';
            # but when reaching the max_episode_steps,there is a next state s' actually.
            if done and episode_steps != args.max_episode_steps:
                dw = True
            else:
                dw = False

            # Take the 'action'ï¼Œbut store the original 'a'ï¼ˆespecially for Betaï¼‰
            replay_buffer.store(s, a, a_logprob, r, s_, dw, done)
            s = s_
            total_steps += 1

            # When the number of transitions in buffer reaches batch_size,then update
            if replay_buffer.count == args.batch_size:
                agent.update(replay_buffer, total_steps)
                replay_buffer.count = 0

            # Evaluate the policy every 'evaluate_freq' steps
            # if total_steps%1000==0:
            #     print("total_steps",total_steps,"reward",r,"s",state_to_print[:3])
                
            if total_steps % args.evaluate_freq == 0:
                evaluate_num += 1
                evaluate_reward = evaluate_policy(args, env_evaluate, agent, state_norm)
                evaluate_rewards.append(evaluate_reward)
                print("evaluate_num:{} \t evaluate_reward:{} \t".format(evaluate_num, evaluate_reward))
                print("total_steps",total_steps,"reward",r,"s",state_to_print)
                writer.add_scalar('step_rewards_{}'.format(env_name), evaluate_rewards[-1], global_step=total_steps)
                  
                # Save the rewards
                if evaluate_num % args.save_freq == 0:
                    np.save('./data_train/PPO_continuous_{}_env_{}_number_{}_seed_{}.npy'.format(args.policy_dist, env_name, number, seed), np.array(evaluate_rewards))
    #è¾“å‡ºæœ€ç»ˆç»“æœ
    print('ans_max',ans_max,'ans_state',ans_state)
    #è¾“å‡ºæœ€åä¸€å›åˆå®ƒçš„è·¯å¾„  reward_record
    
    # agent.actor_loss
    print('len',len(agent.actor_loss))
    # input()
    plt.plot(np.arange(len(agent.actor_loss)), agent.actor_loss)
    plt.ylabel('actor loss')
    plt.xlabel('steps')
    # plt.savefig('figs/actor_loss.png',dpi='1000')
    plt.show()

    plt.plot(np.arange(len(agent.critic_loss)), agent.critic_loss)
    plt.ylabel('critic loss')
    plt.xlabel('steps')
    # plt.savefig('figs/critic_loss.png',dpi='1000')
    plt.show()
    
    print()
    plt.plot(np.arange(len(reward_record)), reward_record)
    plt.ylabel('Reward')
    plt.xlabel('steps')
    # plt.savefig('figs/reward_record40.png')
    plt.show()
    
    # 41 å•ç”¨æˆ·(48,3)
    # 42 å•ç”¨æˆ·(40,3)
    # 43 åŒç”¨æˆ·(40,3),(44,0)
    
    # 45 10 users
    with open("filename_48.txt", "a") as file:
        print('len(path)',len(path))
        file.write(str(path))
        file.close()

if __name__ == '__main__':
    parser = argparse.ArgumentParser("Hyperparameters Setting for PPO-continuous")
    parser.add_argument("--max_train_steps", type=int, default=int(5e5), help=" Maximum number of training steps")
    # ç»è¿‡æµ‹è¯• 700k å°±å·®ä¸å¤š  å°† 3e6æ”¹ä¸º7e5
    parser.add_argument("--evaluate_freq", type=float, default=5e3, help="Evaluate the policy every 'evaluate_freq' steps")
    parser.add_argument("--save_freq", type=int, default=20, help="Save frequency")
    parser.add_argument("--policy_dist", type=str, default="Gaussian", help="Beta or Gaussian")
    parser.add_argument("--batch_size", type=int, default=2048, help="Batch size")
    parser.add_argument("--mini_batch_size", type=int, default=64, help="Minibatch size")
    parser.add_argument("--hidden_width", type=int, default=64, help="The number of neurons in hidden layers of the neural network")
    parser.add_argument("--lr_a", type=float, default=3e-4, help="Learning rate of actor")
    parser.add_argument("--lr_c", type=float, default=3e-4, help="Learning rate of critic")
    parser.add_argument("--gamma", type=float, default=0.99, help="Discount factor")
    parser.add_argument("--lamda", type=float, default=0.95, help="GAE parameter")
    parser.add_argument("--epsilon", type=float, default=0.2, help="PPO clip parameter")
    parser.add_argument("--K_epochs", type=int, default=10, help="PPO parameter")
    parser.add_argument("--use_adv_norm", type=bool, default=True, help="Trick 1:advantage normalization")
    parser.add_argument("--use_state_norm", type=bool, default=True, help="Trick 2:state normalization")
    parser.add_argument("--use_reward_norm", type=bool, default=False, help="Trick 3:reward normalization")
    parser.add_argument("--use_reward_scaling", type=bool, default=True, help="Trick 4:reward scaling")
    parser.add_argument("--entropy_coef", type=float, default=0.01, help="Trick 5: policy entropy")
    parser.add_argument("--use_lr_decay", type=bool, default=True, help="Trick 6:learning rate Decay")
    parser.add_argument("--use_grad_clip", type=bool, default=True, help="Trick 7: Gradient clip")
    parser.add_argument("--use_orthogonal_init", type=bool, default=True, help="Trick 8: orthogonal initialization")
    parser.add_argument("--set_adam_eps", type=float, default=True, help="Trick 9: set Adam epsilon=1e-5")
    parser.add_argument("--use_tanh", type=float, default=True, help="Trick 10: tanh activation function")

    args = parser.parse_args()

    env_name = ['test','BipedalWalker-v3', 'HalfCheetah-v2', 'Hopper-v2', 'Walker2d-v2']
    env_index = 0
    main(args, env_name=env_name[env_index], number=55, seed=10)
    
    # 26 yidong
    # 27 buyi dong

    # 28 -1 -1 -1 (48,20)
    # 29 -1 -1 -1 (48,20) (48,-20)
    
    
    
    
    
    # ***************
    # ç”»å‡ºè½¨è¿¹å›¾ğŸ—ºï¸ï¼Œ
    # å…ˆè€ƒè™‘å•ç”¨æˆ·æƒ…å†µï¼Œçœ‹ä¸€ä¸‹å®ƒçš„æœ€ç»ˆä¼˜åŒ–è½¨è¿¹å¦‚ä½•
    # ç„¶åè€ƒè™‘é›†ä¸­çš„å¤šç”¨æˆ·æƒ…å†µï¼Œç”»ä¸€ä¸ªåœˆï¼ŒæŠŠæœ€ç»ˆä¼˜åŒ–è½¨è¿¹å±•ç°
    
    